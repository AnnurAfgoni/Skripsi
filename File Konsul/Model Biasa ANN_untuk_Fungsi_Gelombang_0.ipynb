{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b3e00ce",
   "metadata": {},
   "source": [
    "* Feature Tanpa Scaling.\n",
    "* Target Tanpa Scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9990e3",
   "metadata": {
    "id": "7c9990e3"
   },
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c09ff43",
   "metadata": {
    "id": "3c09ff43"
   },
   "outputs": [],
   "source": [
    "# import necessary module\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430e44f3",
   "metadata": {},
   "source": [
    "## Model Biasa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bf3bee7",
   "metadata": {
    "id": "9bf3bee7"
   },
   "outputs": [],
   "source": [
    "def model(n_input, n_target):\n",
    "    \"\"\"\n",
    "    Fungsi untuk membuat model\n",
    "    \n",
    "    arguments:\n",
    "        n_input = jumlah feature dari data input\n",
    "        n_outut = jumlah target\n",
    "        \n",
    "    return:\n",
    "        model\n",
    "    \"\"\"\n",
    "    # initializer \n",
    "    initializer = tf.keras.initializers.GlorotUniform(seed=None)\n",
    "    mse = tf.keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mean_squared_error\")\n",
    "    aktivasi = \"relu\"\n",
    "\n",
    "    # input layer\n",
    "    visible = Input(shape=(n_input))\n",
    "    \n",
    "    # branch 1\n",
    "    x1 = Dense(100, kernel_initializer = initializer, activation = aktivasi)(visible)\n",
    "    x1 = Dense(75, kernel_initializer = initializer, activation = aktivasi)(x1)\n",
    "    x1 = Dense(50, kernel_initializer = initializer, activation = aktivasi)(x1)\n",
    "    x1 = Dense(n_target, kernel_initializer = initializer, activation = \"tanh\")(x1)    \n",
    "\n",
    "    # model\n",
    "    model = Model(inputs = visible, \n",
    "                  outputs = x1)\n",
    "    \n",
    "    #compile model\n",
    "    model.compile(loss = mse,\n",
    "                  optimizer = 'nadam')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1bf909",
   "metadata": {
    "id": "ce1bf909"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c496e1c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 456
    },
    "id": "7c496e1c",
    "outputId": "ab2ee7cf-9d44-4a9e-e09f-0000e234baea"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "      <th>c4</th>\n",
       "      <th>c5</th>\n",
       "      <th>c6</th>\n",
       "      <th>c7</th>\n",
       "      <th>c8</th>\n",
       "      <th>c9</th>\n",
       "      <th>c10</th>\n",
       "      <th>...</th>\n",
       "      <th>c91</th>\n",
       "      <th>c92</th>\n",
       "      <th>c93</th>\n",
       "      <th>c94</th>\n",
       "      <th>c95</th>\n",
       "      <th>c96</th>\n",
       "      <th>c97</th>\n",
       "      <th>c98</th>\n",
       "      <th>c99</th>\n",
       "      <th>c100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>-2.564005</td>\n",
       "      <td>-2.974074</td>\n",
       "      <td>-2.449338</td>\n",
       "      <td>-2.563678</td>\n",
       "      <td>-2.571632</td>\n",
       "      <td>-2.661665</td>\n",
       "      <td>-2.746351</td>\n",
       "      <td>-2.331729</td>\n",
       "      <td>-2.401345</td>\n",
       "      <td>-2.434173</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.686741</td>\n",
       "      <td>-2.958202</td>\n",
       "      <td>-2.2606</td>\n",
       "      <td>-2.341488</td>\n",
       "      <td>-2.777364</td>\n",
       "      <td>-2.575247</td>\n",
       "      <td>-2.307158</td>\n",
       "      <td>-2.289519</td>\n",
       "      <td>-2.530523</td>\n",
       "      <td>-1.819348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1567</th>\n",
       "      <td>-2.563005</td>\n",
       "      <td>-2.973074</td>\n",
       "      <td>-2.443338</td>\n",
       "      <td>-2.557678</td>\n",
       "      <td>-2.536632</td>\n",
       "      <td>-2.626665</td>\n",
       "      <td>-2.534351</td>\n",
       "      <td>-2.119729</td>\n",
       "      <td>-1.133345</td>\n",
       "      <td>-1.166173</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.685741</td>\n",
       "      <td>-2.957202</td>\n",
       "      <td>-2.2546</td>\n",
       "      <td>-2.335488</td>\n",
       "      <td>-2.742364</td>\n",
       "      <td>-2.540247</td>\n",
       "      <td>-2.095158</td>\n",
       "      <td>-2.077519</td>\n",
       "      <td>-1.262523</td>\n",
       "      <td>-0.551348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3296</th>\n",
       "      <td>-2.561005</td>\n",
       "      <td>-2.971074</td>\n",
       "      <td>-2.435338</td>\n",
       "      <td>-2.549678</td>\n",
       "      <td>-2.488632</td>\n",
       "      <td>-2.578665</td>\n",
       "      <td>-2.246351</td>\n",
       "      <td>-1.831729</td>\n",
       "      <td>0.595655</td>\n",
       "      <td>0.562827</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.683741</td>\n",
       "      <td>-2.955202</td>\n",
       "      <td>-2.2466</td>\n",
       "      <td>-2.327488</td>\n",
       "      <td>-2.694364</td>\n",
       "      <td>-2.492247</td>\n",
       "      <td>-1.807158</td>\n",
       "      <td>-1.789519</td>\n",
       "      <td>0.466477</td>\n",
       "      <td>1.177652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4234</th>\n",
       "      <td>-2.561005</td>\n",
       "      <td>-2.971074</td>\n",
       "      <td>-2.431338</td>\n",
       "      <td>-2.545678</td>\n",
       "      <td>-2.462632</td>\n",
       "      <td>-2.552665</td>\n",
       "      <td>-2.090351</td>\n",
       "      <td>-1.675729</td>\n",
       "      <td>1.533655</td>\n",
       "      <td>1.500827</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.683741</td>\n",
       "      <td>-2.955202</td>\n",
       "      <td>-2.2426</td>\n",
       "      <td>-2.323488</td>\n",
       "      <td>-2.668364</td>\n",
       "      <td>-2.466247</td>\n",
       "      <td>-1.651158</td>\n",
       "      <td>-1.633519</td>\n",
       "      <td>1.404477</td>\n",
       "      <td>2.115652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2375</th>\n",
       "      <td>-2.562005</td>\n",
       "      <td>-2.972074</td>\n",
       "      <td>-2.440338</td>\n",
       "      <td>-2.554678</td>\n",
       "      <td>-2.514632</td>\n",
       "      <td>-2.604665</td>\n",
       "      <td>-2.400351</td>\n",
       "      <td>-1.985729</td>\n",
       "      <td>-0.325345</td>\n",
       "      <td>-0.358173</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.684741</td>\n",
       "      <td>-2.956202</td>\n",
       "      <td>-2.2516</td>\n",
       "      <td>-2.332488</td>\n",
       "      <td>-2.720364</td>\n",
       "      <td>-2.518247</td>\n",
       "      <td>-1.961158</td>\n",
       "      <td>-1.943519</td>\n",
       "      <td>-0.454523</td>\n",
       "      <td>0.256652</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            c1        c2        c3        c4        c5        c6        c7  \\\n",
       "299  -2.564005 -2.974074 -2.449338 -2.563678 -2.571632 -2.661665 -2.746351   \n",
       "1567 -2.563005 -2.973074 -2.443338 -2.557678 -2.536632 -2.626665 -2.534351   \n",
       "3296 -2.561005 -2.971074 -2.435338 -2.549678 -2.488632 -2.578665 -2.246351   \n",
       "4234 -2.561005 -2.971074 -2.431338 -2.545678 -2.462632 -2.552665 -2.090351   \n",
       "2375 -2.562005 -2.972074 -2.440338 -2.554678 -2.514632 -2.604665 -2.400351   \n",
       "\n",
       "            c8        c9       c10  ...       c91       c92     c93       c94  \\\n",
       "299  -2.331729 -2.401345 -2.434173  ... -2.686741 -2.958202 -2.2606 -2.341488   \n",
       "1567 -2.119729 -1.133345 -1.166173  ... -2.685741 -2.957202 -2.2546 -2.335488   \n",
       "3296 -1.831729  0.595655  0.562827  ... -2.683741 -2.955202 -2.2466 -2.327488   \n",
       "4234 -1.675729  1.533655  1.500827  ... -2.683741 -2.955202 -2.2426 -2.323488   \n",
       "2375 -1.985729 -0.325345 -0.358173  ... -2.684741 -2.956202 -2.2516 -2.332488   \n",
       "\n",
       "           c95       c96       c97       c98       c99      c100  \n",
       "299  -2.777364 -2.575247 -2.307158 -2.289519 -2.530523 -1.819348  \n",
       "1567 -2.742364 -2.540247 -2.095158 -2.077519 -1.262523 -0.551348  \n",
       "3296 -2.694364 -2.492247 -1.807158 -1.789519  0.466477  1.177652  \n",
       "4234 -2.668364 -2.466247 -1.651158 -1.633519  1.404477  2.115652  \n",
       "2375 -2.720364 -2.518247 -1.961158 -1.943519 -0.454523  0.256652  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input data\n",
    "\n",
    "feature = pd.read_csv(\"coef_fourier_kolom.csv\")\n",
    "feature.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7626aa8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c1</th>\n",
       "      <th>c2</th>\n",
       "      <th>c3</th>\n",
       "      <th>c4</th>\n",
       "      <th>c5</th>\n",
       "      <th>c6</th>\n",
       "      <th>c7</th>\n",
       "      <th>c8</th>\n",
       "      <th>c9</th>\n",
       "      <th>c10</th>\n",
       "      <th>...</th>\n",
       "      <th>c91</th>\n",
       "      <th>c92</th>\n",
       "      <th>c93</th>\n",
       "      <th>c94</th>\n",
       "      <th>c95</th>\n",
       "      <th>c96</th>\n",
       "      <th>c97</th>\n",
       "      <th>c98</th>\n",
       "      <th>c99</th>\n",
       "      <th>c100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3087</th>\n",
       "      <td>0.020048</td>\n",
       "      <td>0.037265</td>\n",
       "      <td>0.049329</td>\n",
       "      <td>0.055424</td>\n",
       "      <td>0.055588</td>\n",
       "      <td>0.050375</td>\n",
       "      <td>0.041036</td>\n",
       "      <td>0.029212</td>\n",
       "      <td>0.016821</td>\n",
       "      <td>0.005729</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034217</td>\n",
       "      <td>0.063591</td>\n",
       "      <td>0.084156</td>\n",
       "      <td>0.094539</td>\n",
       "      <td>0.094817</td>\n",
       "      <td>0.085938</td>\n",
       "      <td>0.070007</td>\n",
       "      <td>0.049806</td>\n",
       "      <td>0.028595</td>\n",
       "      <td>0.009600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1207</th>\n",
       "      <td>-0.015843</td>\n",
       "      <td>-0.029645</td>\n",
       "      <td>-0.040811</td>\n",
       "      <td>-0.047856</td>\n",
       "      <td>-0.051404</td>\n",
       "      <td>-0.050910</td>\n",
       "      <td>-0.047152</td>\n",
       "      <td>-0.040459</td>\n",
       "      <td>-0.031876</td>\n",
       "      <td>-0.021693</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.027321</td>\n",
       "      <td>-0.051157</td>\n",
       "      <td>-0.070444</td>\n",
       "      <td>-0.082594</td>\n",
       "      <td>-0.088689</td>\n",
       "      <td>-0.087844</td>\n",
       "      <td>-0.081386</td>\n",
       "      <td>-0.069886</td>\n",
       "      <td>-0.055103</td>\n",
       "      <td>-0.037444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1911</th>\n",
       "      <td>-0.016626</td>\n",
       "      <td>-0.031568</td>\n",
       "      <td>-0.043217</td>\n",
       "      <td>-0.050159</td>\n",
       "      <td>-0.052926</td>\n",
       "      <td>-0.051214</td>\n",
       "      <td>-0.045945</td>\n",
       "      <td>-0.037794</td>\n",
       "      <td>-0.028047</td>\n",
       "      <td>-0.017378</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028549</td>\n",
       "      <td>-0.054236</td>\n",
       "      <td>-0.074264</td>\n",
       "      <td>-0.086184</td>\n",
       "      <td>-0.090916</td>\n",
       "      <td>-0.087982</td>\n",
       "      <td>-0.078951</td>\n",
       "      <td>-0.064980</td>\n",
       "      <td>-0.048239</td>\n",
       "      <td>-0.029822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4929</th>\n",
       "      <td>-0.031169</td>\n",
       "      <td>-0.049488</td>\n",
       "      <td>-0.058296</td>\n",
       "      <td>-0.059683</td>\n",
       "      <td>-0.051953</td>\n",
       "      <td>-0.038063</td>\n",
       "      <td>-0.019691</td>\n",
       "      <td>-0.000340</td>\n",
       "      <td>0.016617</td>\n",
       "      <td>0.024504</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053240</td>\n",
       "      <td>-0.084382</td>\n",
       "      <td>-0.099222</td>\n",
       "      <td>-0.101535</td>\n",
       "      <td>-0.088455</td>\n",
       "      <td>-0.064872</td>\n",
       "      <td>-0.033544</td>\n",
       "      <td>-0.000364</td>\n",
       "      <td>0.028791</td>\n",
       "      <td>0.042198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2909</th>\n",
       "      <td>-0.019292</td>\n",
       "      <td>-0.036151</td>\n",
       "      <td>-0.048220</td>\n",
       "      <td>-0.054532</td>\n",
       "      <td>-0.055237</td>\n",
       "      <td>-0.050712</td>\n",
       "      <td>-0.042150</td>\n",
       "      <td>-0.031009</td>\n",
       "      <td>-0.019086</td>\n",
       "      <td>-0.007989</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032950</td>\n",
       "      <td>-0.061747</td>\n",
       "      <td>-0.082346</td>\n",
       "      <td>-0.093110</td>\n",
       "      <td>-0.094309</td>\n",
       "      <td>-0.086595</td>\n",
       "      <td>-0.071980</td>\n",
       "      <td>-0.052939</td>\n",
       "      <td>-0.032524</td>\n",
       "      <td>-0.013498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            c1        c2        c3        c4        c5        c6        c7  \\\n",
       "3087  0.020048  0.037265  0.049329  0.055424  0.055588  0.050375  0.041036   \n",
       "1207 -0.015843 -0.029645 -0.040811 -0.047856 -0.051404 -0.050910 -0.047152   \n",
       "1911 -0.016626 -0.031568 -0.043217 -0.050159 -0.052926 -0.051214 -0.045945   \n",
       "4929 -0.031169 -0.049488 -0.058296 -0.059683 -0.051953 -0.038063 -0.019691   \n",
       "2909 -0.019292 -0.036151 -0.048220 -0.054532 -0.055237 -0.050712 -0.042150   \n",
       "\n",
       "            c8        c9       c10  ...       c91       c92       c93  \\\n",
       "3087  0.029212  0.016821  0.005729  ...  0.034217  0.063591  0.084156   \n",
       "1207 -0.040459 -0.031876 -0.021693  ... -0.027321 -0.051157 -0.070444   \n",
       "1911 -0.037794 -0.028047 -0.017378  ... -0.028549 -0.054236 -0.074264   \n",
       "4929 -0.000340  0.016617  0.024504  ... -0.053240 -0.084382 -0.099222   \n",
       "2909 -0.031009 -0.019086 -0.007989  ... -0.032950 -0.061747 -0.082346   \n",
       "\n",
       "           c94       c95       c96       c97       c98       c99      c100  \n",
       "3087  0.094539  0.094817  0.085938  0.070007  0.049806  0.028595  0.009600  \n",
       "1207 -0.082594 -0.088689 -0.087844 -0.081386 -0.069886 -0.055103 -0.037444  \n",
       "1911 -0.086184 -0.090916 -0.087982 -0.078951 -0.064980 -0.048239 -0.029822  \n",
       "4929 -0.101535 -0.088455 -0.064872 -0.033544 -0.000364  0.028791  0.042198  \n",
       "2909 -0.093110 -0.094309 -0.086595 -0.071980 -0.052939 -0.032524 -0.013498  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# target data\n",
    "\n",
    "target = pd.read_csv(\"coef_ekspansi.csv\")\n",
    "target.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f43771a",
   "metadata": {},
   "source": [
    "## Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "nXaKjXArhEXj",
   "metadata": {
    "id": "nXaKjXArhEXj"
   },
   "outputs": [],
   "source": [
    "instances_1, feature_dim = feature.shape\n",
    "instances_2, target_dim = target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bd1f1cf",
   "metadata": {
    "id": "4bd1f1cf"
   },
   "outputs": [],
   "source": [
    "# split training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(feature, target, test_size = 0.2, random_state = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cbb0df",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61d3112f",
   "metadata": {
    "id": "61d3112f"
   },
   "outputs": [],
   "source": [
    "# model \n",
    "\n",
    "model = model(feature_dim, target_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ki7-g8sWrOgr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ki7-g8sWrOgr",
    "outputId": "1f947953-65e3-42c6-8bf7-499af055e236"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 100)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 75)                7575      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 50)                3800      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 100)               5100      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26,575\n",
      "Trainable params: 26,575\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f98bbe2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0f98bbe2",
    "outputId": "6eeb8ff5-2293-4cc6-f76e-579ed2e3ac18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "540/540 [==============================] - 2s 2ms/step - loss: 0.0113 - val_loss: 0.0092\n",
      "Epoch 2/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 3/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 4/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 5/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 6/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 7/250\n",
      "540/540 [==============================] - 1s 1ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 8/250\n",
      "540/540 [==============================] - 1s 1ms/step - loss: 0.0094 - val_loss: 0.0093\n",
      "Epoch 9/250\n",
      "540/540 [==============================] - 1s 1ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 10/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 11/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 12/250\n",
      "540/540 [==============================] - 1s 1ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 13/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 14/250\n",
      "540/540 [==============================] - 1s 1ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 15/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 16/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 17/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0094 - val_loss: 0.0093\n",
      "Epoch 18/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 19/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 20/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 21/250\n",
      "540/540 [==============================] - 1s 1ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 22/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 23/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 24/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 25/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 26/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 27/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 28/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 29/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 30/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 31/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 32/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0094 - val_loss: 0.0092\n",
      "Epoch 33/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 34/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 35/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 36/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 37/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 38/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 39/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 40/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 41/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 42/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 43/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 44/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 45/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 46/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 47/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 48/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 49/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 50/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 51/250\n",
      "540/540 [==============================] - 1s 1ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 52/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 53/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 54/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 55/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 56/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 57/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 58/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 59/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 60/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 61/250\n",
      "540/540 [==============================] - 1s 1ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 62/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 63/250\n",
      "540/540 [==============================] - 1s 1ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 64/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 65/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 66/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 67/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 68/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 69/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 70/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 71/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0094 - val_loss: 0.0095\n",
      "Epoch 72/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 73/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 74/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 75/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 76/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0094 - val_loss: 0.0093\n",
      "Epoch 77/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 78/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 79/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 80/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 81/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 82/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 83/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 84/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 85/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 86/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 87/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 88/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 89/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 90/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 91/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 92/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0094 - val_loss: 0.0092\n",
      "Epoch 93/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 94/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 95/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 96/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 97/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 98/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 99/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 100/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 101/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 102/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 103/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 104/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 105/250\n",
      "540/540 [==============================] - 1s 3ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 106/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 107/250\n",
      "540/540 [==============================] - 2s 3ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 108/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 109/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 110/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 111/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 112/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 113/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 114/250\n",
      "540/540 [==============================] - 1s 3ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 115/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 116/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 117/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 118/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 119/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 120/250\n",
      "540/540 [==============================] - 1s 3ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 121/250\n",
      "540/540 [==============================] - 1s 3ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 122/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 123/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 124/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 125/250\n",
      "540/540 [==============================] - 1s 3ms/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 126/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 127/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 128/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 129/250\n",
      "540/540 [==============================] - 1s 3ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 130/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 131/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 132/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 133/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 134/250\n",
      "540/540 [==============================] - 1s 3ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 135/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 136/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 137/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 138/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 139/250\n",
      "540/540 [==============================] - 1s 3ms/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 140/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 141/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 142/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 143/250\n",
      "540/540 [==============================] - 1s 3ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 144/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 145/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 146/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 147/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 148/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 149/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 150/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 151/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 152/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 153/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 154/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 155/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 156/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 157/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 158/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 159/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 160/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0094 - val_loss: 0.0093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 161/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 162/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 163/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 164/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 165/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 166/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 167/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 168/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 169/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 170/250\n",
      "540/540 [==============================] - 1s 3ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 171/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 172/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 173/250\n",
      "540/540 [==============================] - 1s 3ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 174/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 175/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 176/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 177/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0094 - val_loss: 0.0092\n",
      "Epoch 178/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 179/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 180/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 181/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 182/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 183/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 184/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0094 - val_loss: 0.0092\n",
      "Epoch 185/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 186/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 187/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 188/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 189/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 190/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 191/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 192/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 193/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0094 - val_loss: 0.0093\n",
      "Epoch 194/250\n",
      "540/540 [==============================] - 1s 3ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 195/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 196/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 197/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 198/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 199/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 200/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 201/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 202/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 203/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 204/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 205/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 206/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 207/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 208/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 209/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 210/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 211/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 212/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 213/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 214/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0093\n",
      "Epoch 215/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 216/250\n",
      "540/540 [==============================] - 1s 2ms/step - loss: 0.0095 - val_loss: 0.0092\n",
      "Epoch 217/250\n",
      " 69/540 [==>...........................] - ETA: 0s - loss: 0.0097"
     ]
    }
   ],
   "source": [
    "# training \n",
    "history = model.fit(x = X_train, \n",
    "           y = y_train, \n",
    "           batch_size = 8, \n",
    "           shuffle = True, \n",
    "           validation_data = (X_test, y_test), \n",
    "           epochs = 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd98423e",
   "metadata": {},
   "source": [
    "## Visualisasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5bda08",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c94f509",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = range(1, len(loss) + 1)\n",
    "plt.figure(figsize = (13, 8))\n",
    "plt.style.use(\"seaborn\")\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f953e304",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94675a53",
   "metadata": {
    "id": "94675a53"
   },
   "outputs": [],
   "source": [
    "# prediksi\n",
    "prediction = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3820189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediksi = prediction.flatten()\n",
    "real = y_test.to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f54413",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "f0f54413",
    "outputId": "4f550389-ee8d-49ee-a881-a3c7625370cd"
   },
   "outputs": [],
   "source": [
    "df = {\"predict\": prediksi, \"real\": real}\n",
    "pd.DataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf590d3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "baf590d3",
    "outputId": "1e7d3840-0f10-47f4-8f4d-a51cc0c46a32"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(real, prediksi)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "9decf8e4",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b973492",
   "metadata": {
    "id": "4b973492"
   },
   "source": [
    "# ------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Model 2 ANN untuk Fungsi Gelombang.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
